각 알고리즘의 특징 1-2줄

모델링한 폴더에 들어가서 readme 확인
                                  acc / precision / recall      / f1-score  /auc 순으로 표 만들기
catBoost_72 	= 0.795078 / 0.764211 / 0.722012 / 0.744486 / 0.842025
catBoost_30	= 0.792602 / 0757708 / 0.725038 / 0.740299 / 0.858716
LightGBM_4	= 0.791312 / 0.761258 / 0.713944 / 0.73771 / 0.860846
Xgboost_5           = 0.787597 / 0.781183 / 0.667045 / 0.729744 / 0.860801
RandomForest_38  = 0.782232 / 0.755966 / 0.690872 / 0.721279 / 0.838168

catboost
 - 범주형 변수의 예측모델에 최적화된 모델(범주형 데이터를 처리하는 새로운 방법 제시)
 - 다른 GBM에 비해 과적합 적다 / 범주형 변수에 대해 모델의 정확도&속도 높다 / encoding 작업 없이 모델의 input 가능
###############
 - 기존의 그래디언트 부스팅 알고리즘을 조작하여 타겟 누수(target leakage) 개선
    - 타겟누수: 예측 시점에서 사용할 수 없는 데이터가 데이터셋에 포함되는 오류
 -- 타겟누수 : 
 - 그래디언트 부스팅 모델이 가지고 있는 고질적 문제인 타겟 누수로 인한 prediction shift를 다루기 위해 orbered boosting,
   ordered TS 라는 새로운 알고리즘 제시
 -- ordered boosting : 기존 알고리즘에 대한 순열 기반의 대안
 -- ordered TS : 정보손실 최소화하는 선에서 범주형 피처를 다루는 가장 효율적인 방법
 -- prediction shift : 예측하고자 하는 변수가 데이터셋에 있는 데이터에 가까워지고 실제 값과는 더 멀어질 때 발생

XGBoost
 - 트리 기반의 앙상블 학습모델
 - 뛰어난 예측 성능 / GBM 대비 빠른 수행시간 / 과적합 규제 기능 / 결손값 자체 처리
##############
 - 모델의 학습결과를 명확하게 설명 가능, 그 성능 또한 정형 데이터를 다루는데 있어서 뛰어난 성능을 가짐
 - GBM 대비 빠른 수행시간 : 병렬 처리로 학습, 분류 속도가 빠르다.
 - 과적합 규제(Regularization) : XGBoost는 자체에 과적합 규제 기능으로 강한 내구성 지닌다.
 - 분류와 회귀영역에서 뛰어난 예측 성능 발휘 : CART(Classification and regression tree) 앙상블 모델을 사용
 - Early Stopping(조기 종료) 기능이 있음
 - 결손값을 자체 처리하는 기능

Light-GBM
 - XGBoost보다 학습시간&메모리 사용량 적다 / 기능성의 다양성도 더 많다
 - 카테고리형 피처의 자동 변환이 가능하고 최적 분할이 가능
##############
 - 알고리즘 특징 - 
 - GOSS : 데이터셋의 샘플 수를 감소 / EFB : 데이터셋의 피처 수를 감소의 새로운 알고리즘으로 데이터셋 크기 줄이고
   수행속도 큰폭(압도적)으로 상승, 성능은 XGBoost와 비슷한 수준
 - 과적합이 발생하기 쉽다는 단점
 - 수학적으로 손실에 Upper bound가 있다는 것을 보였고, 실험 결과도 그에 부합

RandomForest
 - 과대 적합(overfitting) 을 방지하기 위해, 최적의 기준 변수를 랜덤하게 선택
 - 일반화 및 성능 우수 / 파라미터 조정 용이 / scale 변환 불필요 / 과적합 잘 안된다.
###############
 - 개별 트리 분석 어렵고 트리 분리가 복잡해지는 경향 / 훈련시 메모리 소모 크다 